{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import math\n",
    "import tempfile\n",
    "import heapq\n",
    "from typing import Union, List\n",
    "\n",
    "class HeapElement:\n",
    "    __slots__ = ('key', 'row', 'file_index', 'ascending')\n",
    "    def __init__(self, key, row, file_index, ascending: bool):\n",
    "        self.key = key\n",
    "        self.row = row\n",
    "        self.file_index = file_index\n",
    "        self.ascending = ascending\n",
    "    def __lt__(self, other):\n",
    "        return self.key < other.key if self.ascending else self.key > other.key\n",
    "\n",
    "def estimate_row_size(file_path: str) -> int:\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.reader(f)\n",
    "        try:\n",
    "            sample = [next(reader) for _ in range(100)]\n",
    "            total_chars = sum(len(','.join(row)) for row in sample)\n",
    "            return max(total_chars // len(sample), 1)\n",
    "        except StopIteration:\n",
    "            return 100\n",
    "\n",
    "def merge_sort_internal(arr: List[List[str]], key_index: int, ascending: bool) -> List[List[str]]:\n",
    "    \"\"\"Implementação didática do Merge-Sort interno.\"\"\"\n",
    "    if len(arr) <= 1:\n",
    "        return arr\n",
    "    \n",
    "    mid = len(arr) // 2\n",
    "    left = merge_sort_internal(arr[:mid], key_index, ascending)\n",
    "    right = merge_sort_internal(arr[mid:], key_index, ascending)\n",
    "\n",
    "    return merge(left, right, key_index, ascending)\n",
    "\n",
    "def merge(left: List[List[str]], right: List[List[str]], key_index: int, ascending: bool) -> List[List[str]]:\n",
    "    result = []\n",
    "    i = j = 0\n",
    "\n",
    "    def key_value(row):\n",
    "        return try_cast(row[key_index])\n",
    "\n",
    "    while i < len(left) and j < len(right):\n",
    "        if ascending:\n",
    "            if key_value(left[i]) <= key_value(right[j]):\n",
    "                result.append(left[i])\n",
    "                i += 1\n",
    "            else:\n",
    "                result.append(right[j])\n",
    "                j += 1\n",
    "        else:\n",
    "            if key_value(left[i]) >= key_value(right[j]):\n",
    "                result.append(left[i])\n",
    "                i += 1\n",
    "            else:\n",
    "                result.append(right[j])\n",
    "                j += 1\n",
    "\n",
    "    result.extend(left[i:])\n",
    "    result.extend(right[j:])\n",
    "    return result\n",
    "\n",
    "def try_cast(value: str):\n",
    "    try:\n",
    "        return float(value)\n",
    "    except ValueError:\n",
    "        return value.lower()\n",
    "\n",
    "def write_sorted_chunk(\n",
    "    chunk: List[List[str]],\n",
    "    key_index: int,\n",
    "    ascending: bool,\n",
    "    temp_dir: str\n",
    ") -> str:\n",
    "    # Ordena o chunk usando merge_sort interno\n",
    "    sorted_chunk = merge_sort_internal(chunk, key_index, ascending)\n",
    "    temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=\".csv\", mode='w', newline='', encoding='utf-8', dir=temp_dir)\n",
    "    writer = csv.writer(temp_file)\n",
    "    writer.writerows(sorted_chunk)\n",
    "    temp_file.close()\n",
    "    return temp_file.name\n",
    "\n",
    "def merge_chunks_multi_pass(\n",
    "    input_files: List[str],\n",
    "    output_file: str,\n",
    "    key_index: int,\n",
    "    ascending: bool,\n",
    "    header: List[str],\n",
    "    temp_dir: str,\n",
    "    merge_factor: int = 10\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Realiza merge multi-pass para escalabilidade, mesclando até merge_factor arquivos por passada.\n",
    "    \"\"\"\n",
    "    current_files = input_files\n",
    "\n",
    "    while len(current_files) > 1:\n",
    "        next_round_files = []\n",
    "        for i in range(0, len(current_files), merge_factor):\n",
    "            group = current_files[i:i + merge_factor]\n",
    "            merged_file = tempfile.NamedTemporaryFile(delete=False, suffix=\".csv\", mode='w', newline='', encoding='utf-8', dir=temp_dir).name\n",
    "            merge_files_heap(group, merged_file, key_index, ascending)\n",
    "            next_round_files.append(merged_file)\n",
    "            # Remove arquivos antigos após merge\n",
    "            for f in group:\n",
    "                os.remove(f)\n",
    "        current_files = next_round_files\n",
    "\n",
    "    # Renomeia o arquivo final para o nome de saída desejado\n",
    "    os.rename(current_files[0], output_file)\n",
    "\n",
    "    # Escreve o cabeçalho no arquivo final (precisa reabrir e reescrever)\n",
    "    with open(output_file, 'r', newline='', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as f:\n",
    "        f.write(','.join(header) + '\\n')\n",
    "        f.writelines(lines)\n",
    "\n",
    "def merge_files_heap(\n",
    "    input_files: List[str],\n",
    "    output_file: str,\n",
    "    key_index: int,\n",
    "    ascending: bool\n",
    ") -> None:\n",
    "    files = [open(fname, 'r', newline='', encoding='utf-8') for fname in input_files]\n",
    "    readers = [csv.reader(f) for f in files]\n",
    "    heap = []\n",
    "\n",
    "    for idx, reader in enumerate(readers):\n",
    "        row = next(reader, None)\n",
    "        if row:\n",
    "            heapq.heappush(heap, HeapElement(try_cast(row[key_index]), row, idx, ascending))\n",
    "\n",
    "    with open(output_file, 'w', newline='', encoding='utf-8') as out_f:\n",
    "        writer = csv.writer(out_f)\n",
    "        while heap:\n",
    "            element = heapq.heappop(heap)\n",
    "            writer.writerow(element.row)\n",
    "            next_row = next(readers[element.file_index], None)\n",
    "            if next_row:\n",
    "                heapq.heappush(heap, HeapElement(try_cast(next_row[key_index]), next_row, element.file_index, ascending))\n",
    "\n",
    "    for f in files:\n",
    "        f.close()\n",
    "\n",
    "def external_merge_sort_optimized(\n",
    "    file_path: str,\n",
    "    key_column: Union[str, int],\n",
    "    order: str = \"asc\",\n",
    "    buffer_size_mb: int = 100,\n",
    "    output_file: str = \"sorted_output.csv\",\n",
    "    temp_dir: str = None,\n",
    "    merge_factor: int = 10\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Ordenação externa baseada em Merge-Sort com merge multi-pass e merge-sort interno.\n",
    "\n",
    "    Args:\n",
    "        file_path: Caminho do arquivo CSV original.\n",
    "        key_column: Nome ou índice da coluna chave.\n",
    "        order: 'asc' ou 'desc'.\n",
    "        buffer_size_mb: Tamanho do buffer em MB para leitura dos chunks.\n",
    "        output_file: Caminho do arquivo final ordenado.\n",
    "        temp_dir: Diretório para arquivos temporários (cria um temporário se None).\n",
    "        merge_factor: Quantidade de arquivos mesclados por passada no merge multi-pass.\n",
    "    \"\"\"\n",
    "    if order not in [\"asc\", \"desc\"]:\n",
    "        raise ValueError(\"A ordem deve ser 'asc' ou 'desc'.\")\n",
    "\n",
    "    ascending = (order == \"asc\")\n",
    "    row_size = estimate_row_size(file_path)\n",
    "    rows_per_chunk = max((buffer_size_mb * 1024 * 1024) // row_size, 1)\n",
    "\n",
    "    # Cria diretório temporário se não informado\n",
    "    if temp_dir is None:\n",
    "        temp_dir = tempfile.mkdtemp(prefix=\"extsort_\")\n",
    "    else:\n",
    "        os.makedirs(temp_dir, exist_ok=True)\n",
    "\n",
    "    temp_files = []\n",
    "    with open(file_path, 'r', encoding='utf-8', newline='') as f:\n",
    "        reader = csv.reader(f)\n",
    "        header = next(reader)\n",
    "        key_index = header.index(key_column) if isinstance(key_column, str) else key_column\n",
    "\n",
    "        chunk = []\n",
    "        for row in reader:\n",
    "            chunk.append(row)\n",
    "            if len(chunk) >= rows_per_chunk:\n",
    "                temp_files.append(write_sorted_chunk(chunk, key_index, ascending, temp_dir))\n",
    "                chunk = []\n",
    "        if chunk:\n",
    "            temp_files.append(write_sorted_chunk(chunk, key_index, ascending, temp_dir))\n",
    "\n",
    "    # Merge multi-pass para escalabilidade\n",
    "    merge_chunks_multi_pass(temp_files, output_file, key_index, ascending, header, temp_dir, merge_factor)\n",
    "\n",
    "    # Remove diretório temporário se estiver vazio\n",
    "    try:\n",
    "        os.rmdir(temp_dir)\n",
    "    except OSError:\n",
    "        pass\n",
    "\n",
    "    return output_file\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import random\n",
    "\n",
    "    # Criar arquivo CSV de teste grande\n",
    "    test_file = \"test_large.csv\"\n",
    "    num_rows = 100000  # Ajuste conforme sua RAM e necessidade\n",
    "\n",
    "    if not os.path.exists(test_file):\n",
    "        print(f\"Criando arquivo de teste '{test_file}' com {num_rows} linhas...\")\n",
    "        with open(test_file, 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(['id', 'nome', 'idade', 'salario'])\n",
    "            for i in range(num_rows):\n",
    "                writer.writerow([\n",
    "                    random.randint(1, 1000000),\n",
    "                    f\"Nome{i % 1000}\",\n",
    "                    random.randint(18, 70),\n",
    "                    round(random.uniform(1000, 10000), 2)\n",
    "                ])\n",
    "        print(\"Arquivo criado.\")\n",
    "\n",
    "    # Executar ordenação externa\n",
    "    print(\"Iniciando ordenação externa...\")\n",
    "    sorted_file = external_merge_sort_optimized(\n",
    "        file_path=test_file,\n",
    "        key_column='id',\n",
    "        order='asc',\n",
    "        buffer_size_mb=10,  # Ajuste conforme memória disponível\n",
    "        output_file=\"sorted_test_large.csv\",\n",
    "        merge_factor=20  # Ajuste para merge multi-pass\n",
    "    )\n",
    "    print(f\"Ordenação concluída. Arquivo ordenado salvo em: {sorted_file}\")\n",
    "    # Opcional: remover arquivo de teste após uso\n",
    "    # os.remove(test_file)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
